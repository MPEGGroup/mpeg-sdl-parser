import type { NodeType, TreeCursor } from "@lezer/common";
import { completionRulesByParentMap } from "./completion-rules.ts";
import getLogger from "../util/logger.ts";

// Generated by Lezer from the SDL grammar
import * as TokenTypeId from "../lezer/parser.terms.ts";
import { parser } from "../lezer/parser.ts";

const nodeTypes = parser.nodeSet.types;

const logger = getLogger("getPotentialTokenTypeIds");

/**
 * Given a TreeCursor positioned at a node, returns the potential token types at that position.
 *
 * @param cursor The TreeCursor positioned at a node.
 *
 * @returns An array of potential NodeType IDs, or undefined if none found.
 */
export function getPotentialTokenTypeIds(
  cursor: TreeCursor,
): number[] | undefined {

  // No potential tokens if currently on a comment
  if (cursor.type.id === TokenTypeId.Comment) {
    logger.debug("No potential tokens in comments");
    return undefined;
  }

  // firstly clone the cursor to avoid breaking the traversal of the original cursor
  const parentCursorClone = cursor.node.cursor();
  const siblingCursorClone = cursor.node.cursor();

  // look for parent token type, ignoring whitespace and errors
  const parentTokenTypesIds: number[] = [];

  while (parentCursorClone.parent()) {
    if (
      (parentCursorClone.type.id !== TokenTypeId.Whitespace) &&
      !parentCursorClone.type.isError
    ) {
      parentTokenTypesIds.unshift(parentCursorClone.type.id);
      break;
    }
  }

  if (parentTokenTypesIds.length === 0) {
    logger.debug("No parent token type found, assuming global scope");
    parentTokenTypesIds.push(TokenTypeId.Specification);
  }

  // look for previous sibling token types, ignoring whitespace, errors and comments
  const previousSiblingTokenTypes: NodeType[] = [];

  while (siblingCursorClone.prevSibling()) {
    if (
      (siblingCursorClone.type.id !== TokenTypeId.Whitespace) &&
      !siblingCursorClone.type.isError &&
      (siblingCursorClone.type.id !== TokenTypeId.Comment)) {
      previousSiblingTokenTypes.unshift(siblingCursorClone.type);
    }
  }

  logger.debug(
    "parentTokenTypes: " +
      parentTokenTypesIds.map((id) => nodeTypes[id].name).join(", "),
  );

  if (previousSiblingTokenTypes.length > 0) {
    logger.debug(
      "previousSiblingTokenTypes: " +
        previousSiblingTokenTypes.map((type) => type.name).join(", "),
    );
  } else {
    logger.debug("No previous sibling token type found");
  }

  const potentialCompletionRules = completionRulesByParentMap
    .get(parentTokenTypesIds[parentTokenTypesIds.length - 1]);

  if (!potentialCompletionRules) {
    logger.debug(
      "No expected tokens map found for parentTokenType: " +
        nodeTypes[parentTokenTypesIds[parentTokenTypesIds.length - 1]].name,
    );

    return undefined;
  }

  const potentialTokenTypes = potentialCompletionRules.get(
    previousSiblingTokenTypes.length > 0
      ? [-1, ...previousSiblingTokenTypes.map((type) => type.id)]
      : [-1],
  );
  if (!potentialTokenTypes || potentialTokenTypes.length === 0) {
    logger.debug(
      "No expected tokens found for parentTokenTypes: " +
        parentTokenTypesIds.map((id) => nodeTypes[id].name).join(", ") +
        (previousSiblingTokenTypes.length > 0
          ? " and previousSiblingTokenTypes: " +
            previousSiblingTokenTypes.map((type) => type.name).join(", ")
          : ""),
    );
    return undefined;
  }

  // sort and remove duplicates
  const uniqueSortedTokenTypes = Array.from(new Set(potentialTokenTypes)).sort((a, b) => a - b);
  logger.debug(
    `Potential token types: ${uniqueSortedTokenTypes.join(" ")}`
  );
  return uniqueSortedTokenTypes;
}
